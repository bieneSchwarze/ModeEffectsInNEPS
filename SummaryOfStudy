Abstract / Summary of Study
----------------------------
Educational large-scale studies typically adopt highly standardized settings to collect cognitive data on large samples of respondents. 
Increasing costs alongside dwindling response rates in these studies necessitate exploring alternative assessment strategies such as 
unstandardized web-based testing. Before respective assessment modes can be implemented on a broad scale, their impact on cognitive 
measurements needs to be established. Therefore, an experimental study on N = 17,473 university students from the German National 
Educational Panel Study is presented. Respondents were randomly assigned to a supervised paper-based, a supervised computerized, 
or an unsupervised web-based mode and worked on a test of scientific literacy. Mode-specific effects on selection bias, measurement bias, 
and predictive bias were examined. The results showed a higher response rate in web-based testing as compared to the supervised modes, 
without introducing a pronounced mode-specific selection bias. Analyses of differential test functioning showed systematically larger test 
scores in paper-based testing, particularly among low to medium ability respondents. Prediction bias for web-based testing was observed 
for one out of four criteria. Overall, these results indicate that web-based testing is not strictly equivalent to other assessment modes. 
However, the respective bias introduced by web-based testing was generally small. Thus, web-based assessments seem to be a feasible option 
in cognitive large-scale studies.
